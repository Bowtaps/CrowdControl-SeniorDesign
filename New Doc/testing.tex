% !TEX root = DesignDocument.tex


\chapter{System and Unit Testing}

Crowd Control utilizes multiple sevices across different platforms, requiring
all parts to be operating correctly in order to provide consistent service to
all users. We use system and unit testing to assist developers in identifing 
potential issues and to verify that the product being produced meets
requirements. The goal of system and unit testing is to automate the process to
increase development efficiency and reduce testing errors.

Given the time constraints and scope of Crowd Control, we have been forced to
make compromises in regards to system and unit testing. Implementing and running
automated unit testing, can be as time-consuming as writing production code. In
fact, it is extremely common for a single function of production code to require
many times as many lines of unit testing code to completely test the function.
While automated testing provides many invaluable benefits, it would ultimately be
a hinderance to the project if it means missing critical deadlines and not
achieving crucial project milestones.

In this section, we describe our approach to testing, our goals, and the test
cases that we have developed and implemented thus far. We also discuss the
future plans for testing and how we will be able to devote more time and energy
towards this project in the upcoming months.


\section{Overview}

For this project, we use a combination of ``black-box'' and ``white-box''
testing. This means that there are some aspects of the project that we have
created ourselves and have full control over its execution, and there are others
that we have little control over because it is was provided by a third party.
In practical terms, it means that we can test our own code with the full
knowledge of how it is intended to work, allowing us to design tests that target
potential problem areas. When designing tests that make use of a third-party
service or a library that we have not written ourselves, we can only write tests
that verify that the supplied functionality works as documented.

Because one of the goals of this project is to develop the applications using
native languages and technology, we make use of the most established testing
frameworks for the platform we are testing on. For the iOS application, we use
the testing tools and frameworks built directly into Xcode. For the Android
application, we use Espresso for testing our interface and JUnit for testing our
classes and methods. To test Parse, the third party service we use for data
storage and retrieval, we use unit testing on both Android and iOS, as
interacting with Parse requires a running application on a device. To test
Sinch, the third party service we use for instant messaging between users, we
use the unit testing frameworks we use for testing our Android and iOS
applications since interacting with Sinch also requires a running instance of
the application on a device.

Running unit tests in all of the frameworks above yields a list of binary
results; for each test executed, we receive either a ``pass'' or ``fail''
result. If a test fails, we may also receive a message describing the issue
that caused the test to fail. There exist several conditions that could cause
a test to fail.

\begin{itemize}
	\item An uncaught exception is thrown
	\item A runtime error is encountered
	\item The application crashes
	\item A test assertion fails
\end{itemize}

When designing test cases, we begin with the set of user stories identified at
the beginning of the project. A list of user stories can be found in the section
\ref{userStories}. From these user stories, we build a testing matrix that will
be referenced when writing our automated tests. Tests are then written as the
code they are intended to test is written.

Tests are also to be run relatively frequently. Ideally, developers would enable
the set of tests relevant for the aspect of the project they are currently
working on and run those tests as needed during development and again before the
code is merged in with the rest of the project. Then, the full set of tests
would be run automatically and at a time that would not interfere with
development or execution of the product.


\section{Dependencies}

This project can be broken into several pieces that all work together to form a
single cohesive product. Taken individually, these pieces would be of little
utility. It is important that we test each how well each component interacts
with each other as well as each isolated part as best we can. The four main
components of this project are as follows:

\begin{itemize}
	\item Android client application
	\item iOS client application
	\item Parse service and cloud code
	\item Sinch service
\end{itemize}

Each of these aspects of the project need to be somehow tested, although some
parts can be more accurately tested than others. More thorough requirement
breakdowns of each aspect are described in the subsections below.


\subsection{Android Client Application}

To test our client code running locally on Android devices, we use two testing
frameworks: JUnit for testing Android-independent code and Espresso for testing
navigation and user interface requirements. These frameworks are run from within
our integrated development environment, Android Studio. Additionally, for
interface testing, we require a running emulator or a connected Android device on
which Android Studio can run the unit tests.

While developing unit tests for the JUnit and Espresso frameworks, we use
\href{https://google.github.io/android-testing-support-library/docs/index.html}
{Google's Android testing Support Library Documentation} website. This resource
provides accurate information on installing and using these frameworks in
conjunction with Android Studio. Writing unit tests and best practices, as well
as class documentation are available from this website.


\subsection{iOS Client Application}

To test our client code running locally on iOS devices, we use the testing tools
included in Xcode, our iOS integrated development environment. Backend unit
testing and interface testing are all handled by the same framework. These tests
are run from within Xcode. Interface tests require a running iOS emulator or an
iOS device connected to the computer. Because the unit testing tools used to
test iOS devices are tied to Xcode, this means that the tests can only be run on
a Macintosh computer.

While developing unit tests for iOS devices, we reference \href{https://developer.apple.com/library/ios/documentation/ToolsLanguages/Conceptual/Xcode_Overview/UnitTesting.html}
{iOS Developer Library} website. This resource contains instructions on
designing and setting up unit tests, as well as instructions on running unit
tests. Class documentation can also be found on this website.


\subsection{Parse Service and Cloud Code}

Some aspects of our project exist as ``cloud code'', which is custom server-side
logic executed by our database backend service, Parse. This cloud code is
written in JavaScript and handles database operations that require security,
data integrity, consistency, and efficency. However, because this cloud code
runs on a remote server to which we have limited access and concealed knowledge
of its internal workings, we are forced to test the our code by the only means
we have; using the client applications.

Parse does not supply testing features, and thus we must devise a way to execute
our own unit tests using the tools we have. To test the functionality of our
cloud code and our Parse interaction code, we require these things:

\begin{enumerate}
	\item The Parse libraries installed
	\item Parse interactivity built into the application
	\item An active Internet connection to the Parse service
	\item An emulator/simulator/developer device
	\item The testing framework associated with the client platform being used
	for testing
	\item A Parse application key
	\item A separate Parse database dedicated to testing
\end{enumerate}

In order to run tests on our cloud code, we use both versions of our client (iOS
and Android) to connect to the Parse server, perform database operations by
calling cloud code, then verifying the results. We use the aforementioned
testing frameworks to develop and execute these tests on both platforms. These
tests can be performed on device simulators, device emulators, and physical
developer devices alike.

When using client software to test server software, we must properly configure
the clients to interact with the database as if it were no different than a
production database. Thus, we require that the Parse libraries be included in
the project and that actual Parse functionality built into the application. As
with all Parse-enabled applications, this requires a unique application key be
assigned by Parse and used to access the database.

Because running tests on Parse result in changes to the data in the database we
are testing with, it is crucial that the automated tests be run on an
independent Parse database dedicated to testing. Running such tests on either
the production or development databases could result in irreversable destruction
of data that we cannot afford to jeapordize. In order to avoid this, all unit
tests that involve testing Parse functionality should be executed using a
database where the data is used exclusively for testing.


\subsection{Sinch Service}

Another crucial third party service we utilize is Sinch, an instant messaging
platform that we use in conjunction with Parse. This service does not have cloud
code support, but that is okay since we have no need for cloud code here.
However, it is important that we test how well our client applications connect
to and use the service to ensure that communications run smoothly.

In order to test the functionality of Sinch and how well our applications
interact with it, we require these things:

\begin{enumerate}
	\item The Sinch libraries installed
	\item Sinch functionality built into the application
	\item An active internet connection to Sinch
	\item An emulator/simulator/developer device running the application
	\item The testing framework associated with the client platform being used
	for testing
	\item A Sinch application key
	\item A separate Sinch application dedicated to testing
\end{enumerate}

As with Parse, Sinch is a third party service to which we have limited access
and no knowledge of the implementation details. With the addition of the fact
that we have no cloud code for Sinch to test, the only aspect of this service we
need to test is how our client applications interact with the service.

To test our integration with Sinch, we run tests using our Android and iOS
client applications on either device emulators, device simulators, or physical
development devices. To develop and run these tests, we use the aforementioned
testing frameworks used for testing the client code itself on each platform.
We then run our tests on these devices, which cause the application to connect
to and communicate with Sinch using the provided developer API. Sinch requires
an active Internet connection, so the success of the tests depends on the
testing devices having a stable Internet connection.

Lastly, like Parse, Sinch supplies a unique application key which our clients
use to authenticate with the Sinch servers. All versions of our client
applications use this key and is what allows them to communicate with each
other. Also like Parse, we require that all tests run through Sinch also be run
using a special Sinch application key that is completely separate from the
production and development versions of the application. We must do this in order
to avoid user ID conflicts and to limit any operations that may affect service
uptime and reliability.


\section{Test Setup and Execution}

As with most aspects of software development, test setup and execution requires
two distinct phases: design and execution. As functionality and features are
designed and built, correlating tests are designed concurrently and added to our
testing matrix. After a test has been adequately designed with inputs and
expected outputs defined, the unit tests can be implemented in whatever testing
framework associated with the platform which the tests are targetting.

Setting up unit tests involves referencing the previously designed test
parameters while writing code that will run on the target platform within the
selected testing framework. For example, implementing a unit test that will run
on the Android platform, we create a new unit test class for the portion of the
code we aim to test, following framework usage. The specifics on implementing a
unit test within the constraints of a framework often differ greatly between
frameworks.

Essentially, we create a new testing class that extends some framework-supplied
class. We then create functions that will perform the tests under different
conditions, called test cases. Usually, if our goal is to test the functionality
of a single class, we will create a single test class with the sole purpose of
testing said class.

A full breakdown of our most current testing matrix can be found in the
sections \ref{systemTesting} and \ref{systemIntegrationAnalysis}.


\section{System Testing} \label{systemTesting}

Most of the unit tests our team has designed are related to the functionality of
each individual application as it relates to itself and its own internal
functionality. Below is the portion of our testing matrix that contains tests
that deal exclusively with an individual application ensuring that its own
internal functionality is correct; these tests do not test how well the
application integrates with the other products and services in this project.

\begin{tabularx}{\linewidth}{|X|X|X|}
\hline
\textbf{Requirements} & \textbf{Test Case} & \textbf{Expected} \\

\hline
Log in with email & Valid credentials & Success \\
                  & Invalid email & Failure \\
                  & Invalid password & Failure \\
\hline
Log in with Facebook & Valid credentials & Success \\
                     & Invalid credentials & Failure \\
\hline
Log in with Twitter & Valid credentials & Success \\
                    & Invalid credentials & Failure \\
\hline
Sign up with email & Email already in use & Message \\
                   & Valid credentials & Success \\
                   & Invalid password & Failure \\
                   & Unconfirmed password & Failure \\
                   & Invalid email & Failure + Message \\
                   & Whitespace-only username & Failure + Message \\
                   & Empty username & Failure + Message \\
                   & Empty email & Failure + Message \\
                   & Empty password & Failure + Message \\
\hline
Create a group & No name & Message \\
               & Whitespace-only name & Failure + Message \\
               & No description & Success + Message \\
               & Whitespace-only description & Success + Message \\
               & Else & Success + trim whitespace + collapse whitepace in name \\
\hline
Search for users & Enter display name & Display list of matching users or ``no matches'' \\
                 & Enter email & Detect email, no change \\
                 & Enter phone number & Detect phone number, no change \\
                 & Enter incomplete phone number or incomplete email & Display ``no matches'' \\
\hline
Send invite to users to join group & By email unassociated with user & Generate and send email \\
                                   & By email associated with user & Send app notification to user, display invite in-app \\
                                   & By phone number unassociated with user & Detect phone number, no change \\
                                   & By phone number associated with user & Send app notification to user, display invite in-app \\
\hline
Join a public group & Tap ``join group'' & Displays confirmation message (y/n) \\
                    & Accept confirmation message & Message ``request sent'' \\
                    & Decline confirmation message & Dismiss message \\
\hline
Join group via invite & Tap ``join'' & Join group \\
                      & Tap ``delete'' & Delete invitation \\
\hline
Leave a group & Tap ``Leave Group'' & Display confirmation message (y/n) \\
              & Accept confirmation message & If leader, display confirmation to choose new leader (y/n), else leave the current group and return to nearby groups \\
              & Leader accepts confirmation & Display list of group members \\
              & Leader declines confirmation & Leave current group and return to nearby group list \\
              & Decline confirmation message & Dismiss message, do not leave group \\
\hline
Group leader disbands group & Leader taps ``disband group'' & Display confirmation message (y/n) \\
                            & Accept confirmation & Display message to all users and return to join group page \\
                            & Decline confirmation & Dismiss message, do not leave group \\
\hline
Enter display name & Empty string & Highlight, no accept \\
                   & Too short & Highlight, no accept \\
                   & All whitespace & Highlight, no accept \\
                   & Valid display name & Accept, trim whitespace, collapse consecutive whitespace \\
\hline
Sign out of app & Tap ``log out'' & Display confirmation message (y/n) \\
                & Accept confirmation & Log out of the application and display login page \\
                & Decline confirmation & Dismiss message, do not change application state \\
\hline
Send a group message & Emtpy string & Send button disabled \\
                     & Non-empty string & Message sent to all members and immediately displayed in conversation + saved to storage \\
\hline
See locations of group members & Tap on ``map'' tab & Map is displayed, group member locations displayed on map as pins (zoombed out to display all group members) \\
\hline
Opt-out location updates & Toggle ``enable location updates'' off & Display toast/notify message \\
                         & Toggle ``enable location updates'' on & Display confirmation message (y/n) \\
                         & Accept confirmation & Share user location, begin auto location share \\
                         & Decline confirmation & Toggle ``enable location updates'' off \\
\hline
Receive message & Received message & Unread indicator in tab navigation bar \\
\hline
\end{tabularx}




\section{System Integration Analysis} \label{systemIntegrationAnalysis}

In addition to ensuring that every application running on every iOS and Android
device works as expected, it's equally important to test whether or not the
applications communicate as expected with the third-party services and our own
cloud code. Below is the remainder of our testing matrix, which contains test
cases that test how well individual components of our project integrate with the
others and how well the system as a whole works.

% TODO


\section{Risk Analysis}

\subsection{Risk Mitigation}

\section{Successes, Issues and Problems}

During the course of our project's development, we have encountered various
successes, issues, and problems in regards to system and unit tesing. Details on
each of these are listed below, along with a summary on the changes made to our
testing backlog throughout the course of this project.


\subsection{Successes}

The greatest success of our work in unit and system tesing has been in the
development of our testing matrix. Our tesing matrix, while not comprehensive
(as such a matrix requires years of refining to produce), will act as a set of
blueprints when setting up automated tests. At over one hundred test cases
strong, this matrix defines the behavior of our application in many different
possible states. Using this information, we can implement automated unit tests
more efficiently than if we did not have such a matrix.

A secondary success in our testing efforts was in discovering that all of the
necessary tesing frameworks are free and are mature enough to have exhaustive
documentation and support. We have access to a plethora of information that has
aided us in designing and implementing unit and system tests using these native
tools. Additionaly, the tools we are using integrate seamlessly into the IDEs we
are using for development, reducing the number of tools required to use them.

Finally, we have succeeded in designing and implementing a number of unit tests
for both iOS and Android versions of our client applications. As implementing
unit tests takes a significant time investment, the ones we have built provide a
good foundation upon which to build. On iOS for example, many interface tests
related to the login and signup screens have been implemented and are used for
tesing account creation and login. On Android, we have implemented a similar set
of unit tests as we did for iOS.


\subsection{Issues}

One major issue we encountered when designing unit tests was the fact that none
of the team members have had significant experience doing so. Although a number
of us have had a moderate amount of formal education on the subject through
couse and guest lectures, none have designed or implemented automated tesing in
a project of this magnitude. Thus, an enormous part of testing for this project
was devoted to studying testing methods used in other projects and researching
how to use our tools to build these tests.

The second issue encountered was time constraints. Our goals to turn this
project into a viable product fit for the market required that we focus heavily
on progress towards a working prototype that can be demonstrated by the end of
the semester. Automated tesing, although an important development tool, requires
a large time investment overhead that we could not afford if we desired to meet
requirement deadlines and achieve our critical milestones.

When making the concious decision to prioritize our efforts, we agreed that code
separability and maintainability should not be neglected. These two tennants of
software development are heavilty promoted by automated tesing. By choosing to
prioritize separability and maintainability when designing our software, we are
confident that delaying automated tesing will be less of a hurdle when the time
comes to implement them. Regardless, we are aware that by not investing the time
now, we are accruing ``technical debt'', which means that this task will be more
difficult to accomplish the longer we delay.


\subsection{Problems}

The unit and system testing section of this project was not without its own set
of problems. Many of these we did not forsee, as they are problems that have
arisen from our tools or our tesing devices, both of which we have limited
control over.

Xcode, the IDE chosen for iOS development underwent an update that apeared to
cause UI tesing on iOS to become undependable. Often unit tests run using this
tool would immediately report failures without apparently running the test
properly. It is possible that this is a configuration error, but it is one that
we have yet to solve if it is. Other Xcode users have reported similar problems
using these features.

Another problem encountered was in using our hardware devices to run automated
tesing. For the most part, the device simulators and emulators are adequate for
running most tests. In some cases, the devices would not cooperate with the
testing frameworks. These devices would either run too slowly for the tests and
sometimes reject the incoming connection to the IDE. In the end, we decided to
forego solving these technical difficulties to focus on product development and
business-related issues.


\subsection{Changes to the Backlog}

